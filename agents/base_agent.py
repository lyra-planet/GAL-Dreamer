"""
AgentåŸºç±»
æ‰€æœ‰Agentçš„çˆ¶ç±»ï¼Œæä¾›é€šç”¨çš„LLMè°ƒç”¨ã€é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶
"""
import json
import time
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Union
from pydantic import BaseModel, ValidationError

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.messages import HumanMessage, SystemMessage

from utils.config import config
from utils.logger import log
from utils.json_utils import safe_parse_json


# JSONä¿®å¤æç¤ºè¯æ¨¡æ¿
JSON_FIX_PROMPT = """ä½ ä¹‹å‰ç”Ÿæˆçš„JSONæ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ä¿®å¤ã€‚

ã€é”™è¯¯ä¿¡æ¯ã€‘
{error_message}

ã€ä½ ä¹‹å‰ç”Ÿæˆçš„JSONã€‘
{previous_json}

ã€è¦æ±‚ã€‘
1. å¿…é¡»è¿”å›å®Œæ•´çš„JSONæ ¼å¼
2. å¿…é¡»åŒ…å«ä»¥ä¸‹å¿…å¡«å­—æ®µ: {required_fields}
3. ä¸è¦è¾“å‡ºä»»ä½•JSONä¹‹å¤–çš„è§£é‡Šæ–‡å­—
4. ç›´æ¥è¾“å‡ºä¿®å¤åçš„JSON

è¯·é‡æ–°ç”Ÿæˆæ­£ç¡®çš„JSON:
"""


class AgentConfig(BaseModel):
    """Agenté…ç½®æ¨¡å‹"""
    name: str
    system_prompt: str
    human_prompt_template: str
    use_structured_output: bool = True
    max_fix_rounds: int = 4
    max_redo_rounds: int = 2
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None


class BaseAgent(ABC):
    """
    AgentåŸºç±»

    æä¾›ä»¥ä¸‹åŠŸèƒ½:
    - LLMè°ƒç”¨å°è£…
    - JSONæ ¼å¼éªŒè¯å’Œè‡ªåŠ¨ä¿®å¤
    - é”™è¯¯é‡è¯•æœºåˆ¶
    - å†…å®¹å®¡æ ¸é”™è¯¯å¤„ç†
    - é™çº§å“åº”(fallback)
    """

    # å­ç±»éœ€è¦å®šä¹‰è¿™äº›ç±»å±æ€§
    name: str = ""
    system_prompt: str = ""
    human_prompt_template: str = ""
    required_fields: List[str] = []

    def __init__(self, config: Optional[AgentConfig] = None):
        """
        åˆå§‹åŒ–Agent

        Args:
            config: Agenté…ç½®ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨ç±»å±æ€§
        """
        if config:
            self._config = config
        else:
            self._config = AgentConfig(
                name=self.name,
                system_prompt=self.system_prompt,
                human_prompt_template=self.human_prompt_template,
            )

        # åˆå§‹åŒ–LLM
        self._llm = self._create_llm()
        self._parser = JsonOutputParser()

        log.info(f"{self._config.name} åˆå§‹åŒ–å®Œæˆ")

    def _create_llm(self) -> ChatOpenAI:
        """åˆ›å»ºLLMå®ä¾‹"""
        llm_kwargs = {
            "model": config.LLM_MODEL,
            "api_key": config.LLM_API_KEY,
            "base_url": config.LLM_BASE_URL,
            "temperature": self._config.temperature or config.LLM_TEMPERATURE,
            "max_tokens": self._config.max_tokens or config.LLM_MAX_TOKENS,
            "timeout": config.LLM_TIMEOUT,
        }

        # ç»“æ„åŒ–è¾“å‡ºéœ€è¦JSONæ¨¡å¼
        if self._config.use_structured_output:
            llm_kwargs["model_kwargs"] = {"response_format": {"type": "json_object"}}
            log.debug(f"{self._config.name} å¯ç”¨ç»“æ„åŒ–è¾“å‡º")

        return ChatOpenAI(**llm_kwargs)

    def _create_prompt_template(self) -> ChatPromptTemplate:
        """åˆ›å»ºpromptæ¨¡æ¿"""
        return ChatPromptTemplate.from_messages([
            ("system", self._config.system_prompt),
            ("human", self._config.human_prompt_template)
        ])

    def _extract_json(self, response: str) -> Dict[str, Any]:
        """
        ä»å“åº”ä¸­æå–JSON

        Args:
            response: LLMçš„å“åº”æ–‡æœ¬

        Returns:
            è§£æåçš„JSONå­—å…¸

        Raises:
            ValueError: æ— æ³•è§£æJSONæ—¶
        """
        response = response.strip()

        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        if response.startswith("```"):
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œ
            newline_idx = response.find("\n")
            if newline_idx != -1:
                response = response[newline_idx + 1:]
            # ç§»é™¤ç»“å°¾çš„```
            if response.endswith("```"):
                response = response[:-3]
            response = response.strip()

        # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
        parsed = safe_parse_json(response)
        if parsed is not None:
            return parsed

        # å°è¯•æå–JSONéƒ¨åˆ†ï¼ˆå¤„ç†æœ‰é¢å¤–æ–‡å­—çš„æƒ…å†µï¼‰
        start = response.find("{")
        end = response.rfind("}")

        if start == -1 or end == -1 or end <= start:
            raise ValueError(f"å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚å“åº”å†…å®¹: {response[:500]}...")

        json_str = response[start:end + 1]
        result = safe_parse_json(json_str)

        if result is None:
            raise ValueError(f"JSONè§£æå¤±è´¥ã€‚å“åº”å†…å®¹: {response[:500]}...")

        return result

    def _fix_json_output(self, previous_json: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """
        è®©LLMä¿®å¤ä¸æ­£ç¡®çš„JSONè¾“å‡º

        Args:
            previous_json: ä¹‹å‰ç”Ÿæˆçš„JSON
            error_message: éªŒè¯é”™è¯¯ä¿¡æ¯

        Returns:
            ä¿®å¤åçš„JSON
        """
        required_fields_str = ", ".join(self.required_fields) if self.required_fields else "æ‰€æœ‰åŸå§‹å­—æ®µ"

        fix_prompt = JSON_FIX_PROMPT.format(
            error_message=error_message,
            previous_json=json.dumps(previous_json, ensure_ascii=False, indent=2),
            required_fields=required_fields_str
        )

        messages = [
            SystemMessage(content=self._config.system_prompt),
            HumanMessage(content=fix_prompt)
        ]

        try:
            response = self._llm.invoke(messages)
            return self._extract_json(response.content)
        except Exception as e:
            log.error(f"{self._config.name} JSONä¿®å¤å¤±è´¥: {e}")
            return previous_json

    def _is_content_filter_error(self, error_msg: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å†…å®¹å®¡æ ¸é”™è¯¯"""
        error_keywords = [
            "data_inspection_failed",
            "inappropriate content",
            "content_filter",
            "safety_filter",
            "moderation",
        ]
        return any(keyword in error_msg.lower() for keyword in error_keywords)

    def _should_retry(self, round_num: int, last_error: Optional[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­é‡è¯•"""
        return round_num < self._config.max_fix_rounds - 1

    def _handle_content_filter_error(self, round_num: int) -> None:
        """å¤„ç†å†…å®¹å®¡æ ¸é”™è¯¯"""
        log.warning(f"{self._config.name} è§¦å‘å†…å®¹å®¡æ ¸ (ç¬¬{round_num + 1}è½®)")
        if self._should_retry(round_num, None):
            time.sleep(1)  # ç­‰å¾…åé‡è¯•

    def run(self, **kwargs) -> Dict[str, Any]:
        """
        è¿è¡ŒAgentï¼Œå¸¦JSONéªŒè¯å’Œä¿®å¤é‡è¯•æœºåˆ¶

        Args:
            **kwargs: è¾“å…¥å‚æ•°ï¼Œä¼šæ›¿æ¢promptæ¨¡æ¿ä¸­çš„å˜é‡

        Returns:
            Agentè¾“å‡ºçš„JSONå­—å…¸

        Raises:
            RuntimeError: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åä»ç„¶å¤±è´¥
        """
        log.info(f"{self._config.name} å¼€å§‹æ‰§è¡Œ...")
        log.debug(f"è¾“å…¥å‚æ•°: {json.dumps(kwargs, ensure_ascii=False)[:200]}...")

        prompt_template = self._create_prompt_template()
        chain = prompt_template | self._llm

        current_result: Optional[Dict[str, Any]] = None
        last_error: Optional[str] = None

        for round_num in range(self._config.max_fix_rounds):
            try:
                if round_num == 0:
                    # ç¬¬ä¸€è½®ï¼šæ­£å¸¸ç”Ÿæˆ
                    log.info(f"{self._config.name} ç¬¬{round_num + 1}è½®: ç”Ÿæˆä¸­...")
                    response = chain.invoke(kwargs)
                    response_text = response.content
                else:
                    # åç»­è½®ï¼šä¿®å¤æ¨¡å¼
                    log.info(f"{self._config.name} ç¬¬{round_num + 1}è½®: ä¿®å¤ä¸­...")
                    fixed_result = self._fix_json_output(current_result, last_error)
                    response_text = json.dumps(fixed_result, ensure_ascii=False)

                log.debug(f"åŸå§‹å“åº”: {response_text[:500]}...")

                # è§£æJSON
                result = self._extract_json(response_text)
                current_result = result

                # éªŒè¯è¾“å‡º
                validation_result = self._validate_output(result)

                if validation_result is True:
                    log.success(f"{self._config.name} æ‰§è¡ŒæˆåŠŸ (ç¬¬{round_num + 1}è½®)")
                    return result
                else:
                    last_error = str(validation_result)
                    log.warning(f"{self._config.name} éªŒè¯å¤±è´¥: {last_error}")

            except Exception as e:
                error_msg = str(e)

                # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å®¹å®¡æ ¸é”™è¯¯
                if self._is_content_filter_error(error_msg):
                    self._handle_content_filter_error(round_num)
                    if not self._should_retry(round_num, error_msg):
                        log.error(f"{self._config.name} å†…å®¹å®¡æ ¸å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                        return self._get_fallback_response()
                    continue

                last_error = f"æ‰§è¡Œé”™è¯¯: {error_msg}"
                log.error(f"{self._config.name} ç¬¬{round_num + 1}è½®å¼‚å¸¸: {e}")

                if not self._should_retry(round_num, error_msg):
                    raise RuntimeError(f"{self._config.name} æ‰§è¡Œå¤±è´¥: {error_msg}") from e

        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
        log.error(f"{self._config.name} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°({self._config.max_fix_rounds})")
        return self._get_fallback_response()

    def redo_with_feedback(
        self,
        previous_output: Dict[str, Any],
        feedback_issues: List[Any],
        original_kwargs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ ¹æ®ä¸€è‡´æ€§å®¡æŸ¥åé¦ˆé‡åšç”Ÿæˆ

        Args:
            previous_output: ä¹‹å‰çš„è¾“å‡ºç»“æœ
            feedback_issues: åé¦ˆé—®é¢˜åˆ—è¡¨
            original_kwargs: åŸå§‹è¾“å…¥å‚æ•°

        Returns:
            ä¿®å¤åçš„è¾“å‡º
        """
        log.info(f"{self._config.name} æ ¹æ®åé¦ˆé‡åšï¼Œé—®é¢˜æ•°: {len(feedback_issues)}")

        feedback_prompt = self._build_feedback_prompt(previous_output, feedback_issues)

        messages = [
            SystemMessage(content=self._config.system_prompt),
            HumanMessage(content=feedback_prompt)
        ]

        for round_num in range(self._config.max_redo_rounds):
            try:
                log.info(f"{self._config.name} é‡åšç¬¬{round_num + 1}è½®...")

                response = self._llm.invoke(messages)
                result = self._extract_json(response.content)

                validation_result = self._validate_output(result)
                if validation_result is True:
                    log.success(f"{self._config.name} é‡åšæˆåŠŸ!")

                    # è¾“å‡ºæ›´æ”¹æ‘˜è¦
                    self._log_changes(previous_output, result)

                    return result
                else:
                    log.warning(f"{self._config.name} é‡åšéªŒè¯å¤±è´¥: {validation_result}")
                    messages.append(SystemMessage(
                        content=f"è¾“å‡ºä»æœ‰é—®é¢˜: {validation_result}ã€‚è¯·é‡æ–°ä¿®å¤ã€‚"
                    ))

            except Exception as e:
                log.error(f"{self._config.name} é‡åšå¤±è´¥: {e}")
                if round_num >= self._config.max_redo_rounds - 1:
                    log.warning(f"{self._config.name} é‡åšå¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ")
                    return previous_output

        return previous_output

    def _build_feedback_prompt(
        self,
        previous_output: Dict[str, Any],
        feedback_issues: List[Any]
    ) -> str:
        """æ„å»ºåé¦ˆæç¤ºè¯"""
        # è¿‡æ»¤å‡ºé’ˆå¯¹å½“å‰Agentçš„é—®é¢˜
        my_issues = [
            issue for issue in feedback_issues
            if hasattr(issue, 'source_agent') and issue.source_agent == self._config.name
        ]

        if not my_issues:
            my_issues = feedback_issues

        severity_icons = {
            "low": "ğŸŸ¢",
            "medium": "ğŸŸ¡",
            "high": "ğŸŸ ",
            "critical": "ğŸ”´"
        }

        issue_descriptions = []
        for issue in my_issues:
            severity = getattr(issue, 'severity', 'medium')
            icon = severity_icons.get(severity, "âšª")

            desc = f"{icon} [{severity}]\n"
            desc += f"   é—®é¢˜: {getattr(issue, 'description', 'æœªçŸ¥é—®é¢˜')}\n"
            desc += f"   å»ºè®®: {getattr(issue, 'fix_suggestion', 'æ— ')}"

            if hasattr(issue, 'related_field') and issue.related_field:
                desc += f"\n   å­—æ®µ: {issue.related_field}"

            issue_descriptions.append(desc)

        prompt_parts = [
            "ä¸€è‡´æ€§å®¡æŸ¥å‘ç°ä»¥ä¸‹é—®é¢˜ï¼Œè¯·æ ¹æ®åé¦ˆä¿®æ”¹ä¹‹å‰çš„è¾“å‡ºï¼š",
            "",
            "ã€ä¹‹å‰çš„è¾“å‡ºã€‘",
            json.dumps(previous_output, ensure_ascii=False, indent=2),
            "",
            "ã€å‘ç°çš„é—®é¢˜ã€‘",
            *issue_descriptions,
            "",
            "ã€è¦æ±‚ã€‘",
            "1. ä¿æŒä¸åŸè¾“å‡ºä¸€è‡´çš„ç»“æ„",
            "2. åªä¿®æ”¹æœ‰é—®é¢˜çš„éƒ¨åˆ†",
            "3. ç¡®ä¿ä¿®æ”¹åä¸å†è¿åä¹‹å‰çš„è®¾å®š",
            "4. è¾“å‡ºå®Œæ•´çš„JSONæ ¼å¼",
            "",
            "è¯·è¾“å‡ºä¿®å¤åçš„JSON:"
        ]

        return "\n".join(prompt_parts)

    def _log_changes(self, previous_output: Dict[str, Any], new_output: Dict[str, Any]) -> None:
        """è®°å½•å¹¶æ˜¾ç¤ºè¾“å‡ºå˜æ›´"""
        import difflib

        def compare_dicts(prev, new, path=""):
            changes = []

            # æ£€æŸ¥æ–°å¢æˆ–ä¿®æ”¹çš„é”®
            for key in set(list(prev.keys()) + list(new.keys())):
                current_path = f"{path}.{key}" if path else key

                if key not in prev:
                    changes.append(f"  + {current_path}: {new[key]}")
                elif key not in new:
                    changes.append(f"  - {current_path}: {prev[key]}")
                elif prev[key] != new[key]:
                    # ç±»å‹ä¸åŒæˆ–å€¼ä¸åŒ
                    prev_val = prev[key]
                    new_val = new[key]

                    if isinstance(prev_val, dict) and isinstance(new_val, dict):
                        # é€’å½’æ¯”è¾ƒåµŒå¥—å­—å…¸
                        changes.extend(compare_dicts(prev_val, new_val, current_path))
                    elif isinstance(prev_val, list) and isinstance(new_val, list):
                        # åˆ—è¡¨æ¯”è¾ƒ
                        if str(prev_val) != str(new_val):
                            changes.append(f"  ~ {current_path}:")
                            changes.append(f"      æ—§: {prev_val}")
                            changes.append(f"      æ–°: {new_val}")
                    else:
                        # ç®€å•å€¼æ¯”è¾ƒ
                        changes.append(f"  ~ {current_path}:")
                        changes.append(f"      æ—§: {prev_val}")
                        changes.append(f"      æ–°: {new_val}")

            return changes

        changes = compare_dicts(previous_output, new_output)

        if changes:
            log.info(f"ğŸ“ {self._config.name} æ›´æ–°å†…å®¹:")
            for change in changes[:20]:  # æœ€å¤šæ˜¾ç¤º20æ¡æ›´æ”¹
                log.info(f"{change}")
            if len(changes) > 20:
                log.info(f"  ... è¿˜æœ‰ {len(changes) - 20} æ¡æ›´æ”¹")
        else:
            log.info(f"ğŸ“ {self._config.name} æ— å®è´¨æ€§æ›´æ”¹")

    def _validate_output(self, output: Dict[str, Any]) -> Union[bool, str]:
        """
        éªŒè¯è¾“å‡ºæ˜¯å¦æœ‰æ•ˆ

        Args:
            output: Agentè¾“å‡º

        Returns:
            True: éªŒè¯é€šè¿‡
            str: éªŒè¯å¤±è´¥çš„é”™è¯¯ä¿¡æ¯
        """
        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        missing_fields = [f for f in self.required_fields if f not in output]

        if missing_fields:
            return f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {', '.join(missing_fields)}"

        # è°ƒç”¨å­ç±»çš„è‡ªå®šä¹‰éªŒè¯
        custom_result = self.validate_output(output)
        if custom_result is not True:
            return custom_result

        # å¦‚æœå­ç±»å®šä¹‰äº† output_modelï¼Œè¿›è¡Œ Pydantic éªŒè¯
        if hasattr(self, 'output_model') and self.output_model is not None:
            return self._pydantic_validate(output, self.output_model)

        return True

    def _pydantic_validate(self, output: Dict[str, Any], model_class) -> Union[bool, str]:
        """
        ä½¿ç”¨ Pydantic æ¨¡å‹è¿›è¡Œå®Œæ•´éªŒè¯

        Args:
            output: Agentè¾“å‡º
            model_class: Pydantic æ¨¡å‹ç±»

        Returns:
            True: éªŒè¯é€šè¿‡
            str: éªŒè¯å¤±è´¥çš„é”™è¯¯ä¿¡æ¯
        """
        try:
            model_class(**output)
            return True
        except ValidationError as e:
            # è§£æ Pydantic é”™è¯¯ï¼Œæå–æ¸…æ™°çš„ä¿¡æ¯
            errors = e.errors()
            error_msgs = []
            for error in errors:
                loc = " -> ".join(str(x) for x in error['loc'])
                msg = error['msg']
                error_msgs.append(f"{loc}: {msg}")
            return "ç±»å‹éªŒè¯å¤±è´¥:\n" + "\n".join(error_msgs)
        except Exception as e:
            return f"éªŒè¯å¤±è´¥: {str(e)}"

    def validate_output(self, output: Dict[str, Any]) -> Union[bool, str]:
        """
        éªŒè¯è¾“å‡ºçš„è‡ªå®šä¹‰é€»è¾‘

        å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•æ·»åŠ é¢å¤–çš„éªŒè¯é€»è¾‘

        Args:
            output: Agentè¾“å‡º

        Returns:
            True: éªŒè¯é€šè¿‡
            str: éªŒè¯å¤±è´¥çš„é”™è¯¯ä¿¡æ¯
        """
        return True

    def _get_fallback_response(self) -> Dict[str, Any]:
        """
        è·å–é™çº§å“åº”

        å½“Agentå¤±è´¥æ—¶è¿”å›çš„å®‰å…¨é»˜è®¤å€¼ã€‚
        å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•æä¾›ç‰¹å®šé»˜è®¤å€¼ã€‚

        Returns:
            é™çº§å“åº”å­—å…¸
        """
        return {
            "error": f"{self._config.name} execution failed after max retries",
            "fallback": True,
            "agent_name": self._config.name
        }
